{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADB DLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databases.adbddl import ADBDDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_ddl = ADBDDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "SELECT _SiteId, SUM(Value) as total_production\n",
    "telemetry_processed_silver\n",
    "--WHERE _TelemetryType = '17' -- 17 is the telemetry type for Total Flow\n",
    "GROUP BY _SiteId\n",
    "ORDER BY total_production DESC\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "* \n",
    "from system.information_schema.columns \n",
    "where table_catalog = 'system'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adb_ddl.query_lakehouse(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# Initialize connection.\n",
    "conn = st.experimental_connection('snowpark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select t.table_schema,\n",
    "       t.table_name,\n",
    "       c.column_name,\n",
    "       c.ordinal_position,\n",
    "       c.data_type,\n",
    "       case \n",
    "            when c.numeric_precision is not null\n",
    "                then c.numeric_precision\n",
    "            when c.character_maximum_length is not null\n",
    "                then c.character_maximum_length\n",
    "       end as max_length,\n",
    "       c.numeric_scale, \n",
    "       c.is_identity,\n",
    "       c.is_nullable\n",
    "from information_schema.tables t\n",
    "inner join information_schema.columns c on \n",
    "         c.table_schema = t.table_schema and c.table_name = t.table_name   \n",
    "order by table_schema,\n",
    "       table_name,\n",
    "       ordinal_position;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = st.experimental_connection('snowpark')\n",
    "\n",
    "# Perform query.\n",
    "df = conn.query('SELECT * from mytable;', ttl=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the table as a dataframe using the Snowpark Session.\n",
    "@st.cache_data\n",
    "def load_table(conn, query):\n",
    "    with conn.safe_session() as session:\n",
    "        return session.query(query, ttl=600).to_pandas()\n",
    "\n",
    "df = load_table()\n",
    "\n",
    "# Print results.\n",
    "for row in df.itertuples():\n",
    "    st.write(f\"{row.NAME} has a :{row.PET}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ExplorerAgent\n",
    "agent = ExplorerAgent(**{\"database\":\"default\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(\"can you return a dataframe with the most recent record per WELL_HID in telemetry_table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List  # NOQA: UP035\n",
    "\n",
    "import openai\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "@st.cache_data()\n",
    "def create_gpt_completion(ai_model: str, messages: List[dict]) -> dict:\n",
    "    try:\n",
    "        openai.api_key = st.secrets.api_credentials.api_key\n",
    "    except (KeyError, AttributeError):\n",
    "        st.error(st.session_state.locale.empty_api_handler)\n",
    "    logging.info(f\"{messages=}\")\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=ai_model,\n",
    "        messages=messages,\n",
    "        # stream=True,\n",
    "        # temperature=0.7,\n",
    "    )\n",
    "    logging.info(f\"{completion=}\")\n",
    "    return completion\n",
    "\n",
    "\n",
    "def calc_cost(usage: dict) -> None:\n",
    "    total_tokens = usage.get(\"total_tokens\")\n",
    "    prompt_tokens = usage.get(\"prompt_tokens\")\n",
    "    completion_tokens = usage.get(\"completion_tokens\")\n",
    "    st.session_state.total_tokens.append(total_tokens)\n",
    "    # pricing logic: https://openai.com/pricing#language-models\n",
    "    if st.session_state.model == \"gpt-3.5-turbo\":\n",
    "        cost = total_tokens * 0.002 / 1000\n",
    "    else:\n",
    "        cost = (prompt_tokens * 0.03 + completion_tokens * 0.06) / 1000\n",
    "    st.session_state.costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = create_gpt_completion(st.session_state.model, st.session_state.messages)\n",
    "ai_content = completion.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "calc_cost(completion.get(\"usage\"))\n",
    "st.session_state.messages.append({\"role\": \"assistant\", \"content\": ai_content})\n",
    "if ai_content:\n",
    "    show_chat(ai_content, st.session_state.user_text)\n",
    "    st.divider()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HFT Agent Chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "So far we've been using the tools that the agent has access to. These tools are the following:\n",
    "\n",
    "- **Document question answering**: given a document (such as a PDF) in image format, answer a question on this document (Donut)\n",
    "- **Text question answering**: given a long text and a question, answer the question in the text (Flan-T5)\n",
    "- **Unconditional image captioning**: Caption the image! (BLIP)\n",
    "- **Image question answering**: given an image, answer a question on this image (VILT)\n",
    "- **Image segmentation**: given an image and a prompt, output the segmentation mask of that prompt (CLIPSeg)\n",
    "- **Speech to text**: given an audio recording of a person talking, transcribe the speech into text (Whisper)\n",
    "- **Text to speech**: convert text to speech (SpeechT5)\n",
    "- **Zero-shot text classification**: given a text and a list of labels, identify to which label the text corresponds the most (BART)\n",
    "- **Text summarization**: summarize a long text in one or a few sentences (BART)\n",
    "- **Translation**: translate the text into a given language (NLLB)\n",
    "\n",
    "We also support the following community-based tools:\n",
    "\n",
    "- **Text downloader**: to download a text from a web URL\n",
    "- **Text to image**: generate an image according to a prompt, leveraging stable diffusion\n",
    "- **Image transformation**: transforms an image\n",
    "\n",
    "We can therefore use a mix and match of different tools by explaining in natural language what we would like to do.\n",
    "\n",
    "But what about adding new tools? Let's take a look at how to do that \n",
    "\n",
    "### Adding new tools\n",
    "\n",
    "We'll add a very simple tool so that the demo remains simple: we'll use the awesome cataas (Cat-As-A-Service) API to get random cats on each run.\n",
    "\n",
    "We can get a random cat with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub>0.14 git+https://github.com/huggingface/transformers@$transformers_version -q diffusers accelerate datasets torch soundfile sentencepiece opencv-python openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "transformers_version = \"v4.29.0\" #@param [\"main\", \"v4.29.0\"] {allow-input: true}\n",
    "\n",
    "print(f\"Setting up everything with transformers version {transformers_version}\")\n",
    "\n",
    "%pip install huggingface_hub git+https://github.com/huggingface/transformers@v4.29.0 -q diffusers accelerate datasets torch soundfile sentencepiece opencv-python openai streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import HFTAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create .streamlit/secrets.toml file using % and bash commands\n",
    "#from agents import HFTAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HFTAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = agent.run(\"crete a a name for a web service solution that uses larga language models to build a self service big data platform that helps on creating queries and jobs to process data and chat interaction to analyze and visualize data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = agent.run(\"crete a logo of a web service solution that uses larga language models to build a self service big data platform that helps on creating queries and jobs to process data and chat interaction to analyze and visualize data. It is called, LLM Explorer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = agent.run(\"Can you caption the `image`?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"Show me an an image of a capybara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = st.secrets.connections.huggingface.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"databricks/dolly-v2-3b\" # \"mosaicml/mpt-7b\" # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is a quantum computer?\"\n",
    "\n",
    "response = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adb_connect_team",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
